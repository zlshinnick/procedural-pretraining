# DeepMind Math Training Configuration
# Usage: accelerate launch downstream/semantic/deepmind_math.py --config downstream/semantic/configs/deepmind_math.yaml

# Model configuration
gpt2_size: "12_12_768"
pretrained_model: ""
gradient_checkpointing: false
weights_to_transfer:
  - attn
  - ffn
  - ln
weights_to_be_trained:
  - everything

# Dataset configuration
train_dir: downstream/semantic/data/datasets/deepmind_math/mathematics_dataset-v1.0
validation_dir: downstream/semantic/data/datasets/deepmind_math/mathematics_dataset-v1.0/interpolate
test_dir: downstream/semantic/data/datasets/deepmind_math/mathematics_dataset-v1.0/extrapolate
seq_length: 512

# Training parameters
train_batch_size: 8
valid_batch_size: 16
learning_rate: 5.0e-5
weight_decay: 0.01
max_grad_norm: 1.0
gradient_accumulation_steps: 1
lr_scheduler_type: cosine
num_warmup_steps: 0
max_train_steps: 50000
max_eval_steps: 100

# System parameters
seed: 42
mixed_precision: fp16

# Logging and checkpointing
output_dir: output/deepmind_math
logging_steps: 100
save_checkpoint_steps: 5000

# Weights & Biases
wandb_project: math_pretraining
wandb_name: null
wandb_enable: false
note: ""
