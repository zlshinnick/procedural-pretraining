task: dyck
gpt2_size: "12_12_768"
seed: 42
epochs: 1
steps_per_epoch: 4008
max_steps: 501
save_every_steps: 500
save_total_limit: 20
batch_size: 4
gradient_accumulation_steps: 8
k: 64
seq_length: 2048
warmup_steps: 0
lr_schedule: constant
lr: 0.0005
weight_decay: 0.1
wandb_enable: true
wandb_project: procedural_pretraining
wandb_name: 64_dyck
save_dir: pretrained_models/procedural/dyck/k64/
patience: 50
